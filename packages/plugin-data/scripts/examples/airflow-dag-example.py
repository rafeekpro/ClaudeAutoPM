#!/usr/bin/env python3
"""
Airflow DAG Example - Context7 Best Practices

Demonstrates production-ready Airflow patterns from Context7:
- TaskFlow API with @task decorators
- Task dependencies with bitshift operators
- Sensors for external dependencies
- Error handling and retries
- TaskGroups for organization

Source: /apache/airflow (5,854 snippets, trust 9.1)
"""

from datetime import datetime, timedelta
from typing import Dict, List

from airflow.decorators import dag, task
from airflow.operators.python import PythonOperator
from airflow.sensors.external_task import ExternalTaskSensor
from airflow.utils.task_group import TaskGroup

# âœ… CORRECT: Proper default arguments with error handling
default_args = {
    "owner": "data_team",
    "depends_on_past": False,
    "email": ["alerts@example.com"],
    "email_on_failure": True,
    "email_on_retry": False,
    "retries": 3,
    "retry_delay": timedelta(minutes=5),
    "retry_exponential_backoff": True,
    "max_retry_delay": timedelta(hours=1),
}


@dag(
    dag_id="example_etl_pipeline",
    default_args=default_args,
    description="Production ETL pipeline with Context7 patterns",
    schedule_interval="@daily",
    start_date=datetime(2025, 1, 1),
    catchup=False,
    tags=["example", "etl", "context7"],
)
def example_etl_pipeline():
    """
    Example ETL pipeline demonstrating Context7 patterns.

    Stages:
    1. Wait for upstream dependencies (sensors)
    2. Extract data from multiple sources (parallel)
    3. Transform and validate data
    4. Load to warehouse
    5. Send notifications
    """

    # ===================================================================
    # DEPENDENCY CHECKS
    # ===================================================================

    # âœ… CORRECT: ExternalTaskSensor for upstream DAG dependency
    wait_for_upstream = ExternalTaskSensor(
        task_id="wait_for_upstream_dag",
        external_dag_id="upstream_data_collection",
        external_task_id="final_task",
        allowed_states=["success"],
        failed_states=["failed", "skipped"],
        timeout=3600,
        poke_interval=300,
    )

    # ===================================================================
    # EXTRACTION PHASE (PARALLEL)
    # ===================================================================

    @task
    def extract_from_database() -> Dict:
        """
        Extract data from database.

        Context7 Pattern: TaskFlow with @task decorator
        Returns data dict for XCom passing
        """
        print("Extracting from database...")

        # Simulated extraction
        data = {
            "source": "database",
            "records": list(range(100)),
            "count": 100,
            "timestamp": datetime.utcnow().isoformat(),
        }

        print(f"âœ“ Extracted {data['count']} records from database")
        return data

    @task
    def extract_from_api() -> Dict:
        """
        Extract data from external API.

        Context7 Pattern: Parallel extraction with independent @task
        """
        print("Extracting from API...")

        # Simulated API call
        data = {
            "source": "api",
            "records": list(range(100, 150)),
            "count": 50,
            "timestamp": datetime.utcnow().isoformat(),
        }

        print(f"âœ“ Extracted {data['count']} records from API")
        return data

    # ===================================================================
    # TRANSFORMATION PHASE
    # ===================================================================

    @task
    def transform_and_combine(db_data: Dict, api_data: Dict) -> Dict:
        """
        Transform and combine data from multiple sources.

        Context7 Pattern: TaskFlow automatically handles XCom passing
        Parameters match upstream task outputs
        """
        print("Transforming and combining data...")

        combined_records = db_data["records"] + api_data["records"]
        total_count = db_data["count"] + api_data["count"]

        # Apply transformations
        transformed = {
            "records": [x * 2 for x in combined_records],  # Example transformation
            "count": total_count,
            "sources": [db_data["source"], api_data["source"]],
            "transformation": "doubled values",
            "timestamp": datetime.utcnow().isoformat(),
        }

        print(f"âœ“ Transformed {transformed['count']} total records")
        return transformed

    @task
    def validate_data(data: Dict) -> Dict:
        """
        Validate transformed data quality.

        Context7 Pattern: Data quality checks in pipeline
        """
        print("Validating data quality...")

        # Validation checks
        assert data["count"] > 0, "No records to validate"
        assert len(data["records"]) == data["count"], "Record count mismatch"
        assert all(isinstance(x, int) for x in data["records"]), "Invalid record types"

        print(f"âœ“ Validation passed for {data['count']} records")
        return data

    # ===================================================================
    # LOADING PHASE
    # ===================================================================

    @task
    def load_to_warehouse(data: Dict) -> Dict:
        """
        Load validated data to data warehouse.

        Context7 Pattern: Final load task with success confirmation
        """
        print(f"Loading {data['count']} records to warehouse...")

        # Simulated load operation
        loaded_data = {
            **data,
            "loaded_at": datetime.utcnow().isoformat(),
            "status": "success",
        }

        print(f"âœ“ Successfully loaded {data['count']} records")
        return loaded_data

    @task
    def send_success_notification(data: Dict) -> None:
        """
        Send notification on pipeline success.

        Context7 Pattern: Notification task for monitoring
        """
        message = (
            f"ETL Pipeline completed successfully:\n"
            f"- Records processed: {data['count']}\n"
            f"- Sources: {', '.join(data['sources'])}\n"
            f"- Loaded at: {data['loaded_at']}"
        )

        print(f"ðŸ“§ Notification: {message}")
        # In production: send email, Slack message, etc.

    # ===================================================================
    # DAG STRUCTURE WITH CONTEXT7 PATTERNS
    # ===================================================================

    # Context7 Pattern: Sensor before extraction
    sensor_task = wait_for_upstream

    # Context7 Pattern: Parallel extraction (TaskFlow resolves automatically)
    db_data = extract_from_database()
    api_data = extract_from_api()

    # Context7 Pattern: Pass multiple inputs to transformation
    transformed = transform_and_combine(db_data, api_data)

    # Context7 Pattern: Sequential validation and loading
    validated = validate_data(transformed)
    loaded = load_to_warehouse(validated)
    notify = send_success_notification(loaded)

    # Context7 Pattern: Bitshift operator for explicit dependencies
    sensor_task >> [db_data, api_data]
    loaded >> notify


# Instantiate DAG
dag_instance = example_etl_pipeline()

if __name__ == "__main__":
    print("Airflow DAG Example - Context7 Best Practices")
    print("=" * 60)
    print("")
    print("Context7 Patterns Demonstrated:")
    print("1. âœ… TaskFlow API with @task decorators")
    print("2. âœ… Parallel extraction from multiple sources")
    print("3. âœ… ExternalTaskSensor for upstream dependencies")
    print("4. âœ… Automatic XCom passing between tasks")
    print("5. âœ… Bitshift operators for dependencies (>>, <<)")
    print("6. âœ… Error handling with retries and exponential backoff")
    print("7. âœ… Data validation in pipeline")
    print("8. âœ… Notification on completion")
    print("")
    print("Source: /apache/airflow (5,854 snippets, trust 9.1)")
