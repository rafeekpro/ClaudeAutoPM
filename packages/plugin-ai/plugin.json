{
  "name": "@claudeautopm/plugin-ai",
  "version": "2.0.0",
  "schemaVersion": "2.0",
  "displayName": "AI & Machine Learning",
  "description": "Complete AI/ML plugin with Claude, OpenAI, Azure OpenAI, Gemini, LangChain, HuggingFace, and A2A agents. Includes RAG systems, model deployment, prompt engineering, and MLOps patterns with Context7-verified best practices.",
  "category": "ai",
  "metadata": {
    "category": "AI & Machine Learning",
    "author": "ClaudeAutoPM Team",
    "license": "MIT",
    "homepage": "https://github.com/rafeekpro/ClaudeAutoPM",
    "repository": {
      "type": "git",
      "url": "git+https://github.com/rafeekpro/ClaudeAutoPM.git",
      "directory": "packages/plugin-ai"
    },
    "size": "~35 KB (gzipped)",
    "required": false,
    "tags": [
      "ai",
      "ml",
      "machine-learning",
      "anthropic",
      "claude",
      "openai",
      "azure-openai",
      "gemini",
      "langchain",
      "langgraph",
      "huggingface",
      "a2a",
      "vertex-ai",
      "mlflow",
      "rag",
      "llm"
    ]
  },
  "agents": [
    {
      "name": "anthropic-claude-expert",
      "file": "agents/anthropic-claude-expert.md",
      "category": "ai",
      "description": "Anthropic Claude API integration with Messages API, tool use, prompt caching, vision, and streaming. Expert in cost optimization, async patterns, and production deployment.",
      "version": "2.0.0",
      "tags": [
        "anthropic",
        "claude",
        "llm",
        "tool-use",
        "prompt-caching",
        "vision"
      ],
      "mcp": [],
      "context7": [
        "/anthropic/anthropic-sdk-python",
        "/anthropic/anthropic-sdk-typescript",
        "/websites/docs_anthropic"
      ]
    },
    {
      "name": "openai-python-expert",
      "file": "agents/openai-python-expert.md",
      "category": "ai",
      "description": "OpenAI Python SDK integration with GPT models, embeddings, fine-tuning, and assistants API. Expert in chat completions, function calling, vision, audio processing, and production deployment.",
      "version": "2.0.0",
      "tags": [
        "openai",
        "gpt",
        "llm",
        "embeddings",
        "function-calling"
      ],
      "mcp": [],
      "context7": [
        "/openai/openai-python"
      ]
    },
    {
      "name": "azure-openai-expert",
      "file": "agents/azure-openai-expert.md",
      "category": "ai",
      "description": "Azure OpenAI Service integration with enterprise features, managed identities, private endpoints, and compliance. Expert in multi-tenant architecture, cost allocation, and hybrid deployments.",
      "version": "2.0.0",
      "tags": [
        "azure",
        "openai",
        "enterprise",
        "compliance",
        "managed-identity"
      ],
      "mcp": [],
      "context7": [
        "/azure/azure-sdk-for-python/openai",
        "/websites/azure/openai-service"
      ]
    },
    {
      "name": "gemini-api-expert",
      "file": "agents/gemini-api-expert.md",
      "category": "ai",
      "description": "Google Gemini API integration with text generation, multimodal inputs, function calling, and safety controls. Expert in Gemini Pro/Flash models, structured outputs, streaming responses, and production deployment.",
      "version": "2.0.0",
      "tags": [
        "gemini",
        "google",
        "llm",
        "multimodal",
        "safety"
      ],
      "mcp": [],
      "context7": [
        "/google/generative-ai-python"
      ]
    },
    {
      "name": "langchain-expert",
      "file": "agents/langchain-expert.md",
      "category": "ai",
      "description": "LangChain framework expert with LCEL, chains, agents, memory, RAG, and tools. Expert in async patterns, structured outputs, and production optimization.",
      "version": "2.0.0",
      "tags": [
        "langchain",
        "lcel",
        "chains",
        "agents",
        "rag",
        "memory"
      ],
      "mcp": [],
      "context7": [
        "/langchain-ai/langchain",
        "/websites/python_langchain"
      ]
    },
    {
      "name": "langgraph-workflow-expert",
      "file": "agents/langgraph-workflow-expert.md",
      "category": "ai",
      "description": "LangGraph workflow orchestration with state machines, conditional routing, multi-agent collaboration, and graph-based AI workflows. Expert in stateful applications and complex decision trees.",
      "version": "2.0.0",
      "tags": [
        "langgraph",
        "workflow",
        "state-machine",
        "multi-agent",
        "orchestration"
      ],
      "mcp": [],
      "context7": [
        "/langchain-ai/langgraph",
        "/websites/langchain-ai_github_io_langgraph"
      ]
    },
    {
      "name": "google-a2a-expert",
      "file": "agents/google-a2a-expert.md",
      "category": "ai",
      "description": "Google Agent-to-Agent (A2A) protocol expert with Vertex AI Agent Builder, multi-agent orchestration, and agent collaboration. Expert in A2A SDK, ADK workflows, and distributed agent systems.",
      "version": "2.0.0",
      "tags": [
        "a2a",
        "agent-to-agent",
        "vertex-ai",
        "multi-agent",
        "google-cloud"
      ],
      "mcp": [],
      "context7": [
        "/googleapis/google-cloud-python/vertexai",
        "/websites/cloud_google/vertex-ai",
        "/websites/a2aprotocol"
      ]
    },
    {
      "name": "huggingface-expert",
      "file": "agents/huggingface-expert.md",
      "category": "ai",
      "description": "HuggingFace ecosystem expert with Transformers, Datasets, Model Hub, quantization, and inference optimization. Expert in model deployment, fine-tuning, and production serving.",
      "version": "2.0.0",
      "tags": [
        "huggingface",
        "transformers",
        "model-hub",
        "quantization",
        "inference"
      ],
      "mcp": [],
      "context7": [
        "/huggingface/transformers",
        "/huggingface/datasets",
        "/websites/huggingface/docs"
      ]
    }
  ],
  "commands": [
    {
      "name": "rag-setup-scaffold",
      "file": "commands/rag-setup-scaffold.md",
      "category": "ai",
      "priority": "medium",
      "description": "Generate production-ready RAG system with Context7-verified patterns: document ingestion, vector stores, retrieval strategies, RAG chains with RunnablePassthrough",
      "tags": [
        "rag",
        "langchain",
        "retrieval",
        "embeddings",
        "scaffold"
      ],
      "mcpTools": [
        "context7"
      ]
    },
    {
      "name": "ai-model-deployment",
      "file": "commands/ai-model-deployment.md",
      "category": "ai",
      "priority": "medium",
      "description": "Generate production-ready AI model deployment infrastructure with Context7-verified patterns: FastAPI endpoints, MLflow registry, monitoring, caching, rate limiting",
      "tags": [
        "deployment",
        "mlflow",
        "fastapi",
        "production",
        "mlops"
      ],
      "mcpTools": [
        "context7"
      ]
    },
    {
      "name": "openai-optimize",
      "file": "commands/openai-optimize.md",
      "description": "Optimize OpenAI API usage with async operations, batching, caching, and rate limiting for 90% cost reduction",
      "category": "optimization",
      "tags": ["openai", "async", "batching", "caching", "rate-limiting", "optimization"],
      "requiredAgents": ["openai-python-expert"],
      "context7": [
        "/openai/openai-python"
      ]
    },
    {
      "name": "rag-optimize",
      "file": "commands/rag-optimize.md",
      "description": "Optimize RAG systems with vector store optimization, embeddings caching, MMR retrieval, and optimal chunking",
      "category": "optimization",
      "tags": ["rag", "vector-store", "embeddings", "retrieval", "langchain", "optimization"],
      "requiredAgents": ["langgraph-workflow-expert"],
      "context7": [
        "/websites/python_langchain"
      ]
    },
    {
      "name": "llm-optimize",
      "file": "commands/llm-optimize.md",
      "description": "Optimize LLM inference with model selection, prompt engineering, and context management for 90% cost reduction",
      "category": "optimization",
      "tags": ["llm", "model-selection", "prompts", "inference", "tokens", "optimization"],
      "requiredAgents": ["openai-python-expert", "gemini-api-expert"],
      "context7": [
        "/openai/openai-python",
        "/google/generative-ai-python"
      ]
    },
    {
      "name": "anthropic-optimize",
      "file": "commands/anthropic-optimize.md",
      "description": "Optimize Claude API usage with prompt caching, multi-model routing, streaming, and async batching for 70-90% cost reduction",
      "category": "optimization",
      "tags": ["anthropic", "claude", "prompt-caching", "async", "streaming", "optimization"],
      "requiredAgents": ["anthropic-claude-expert"],
      "context7": [
        "/anthropic/anthropic-sdk-python",
        "/anthropic/prompt-caching",
        "/anthropic/streaming"
      ]
    },
    {
      "name": "langchain-optimize",
      "file": "commands/langchain-optimize.md",
      "description": "Optimize LangChain chains with LCEL patterns, async batch processing, semantic caching, and RAG optimization for 60% cost reduction and 50x speedup",
      "category": "optimization",
      "tags": ["langchain", "lcel", "async", "caching", "rag", "optimization"],
      "requiredAgents": ["langchain-expert"],
      "context7": [
        "/langchain-ai/langchain",
        "/websites/python_langchain",
        "/langchain/lcel-patterns"
      ]
    },
    {
      "name": "a2a-setup",
      "file": "commands/a2a-setup.md",
      "description": "Setup Google Agent-to-Agent (A2A) protocol with Vertex AI Agent Builder, multi-agent orchestration, and distributed agent systems",
      "category": "setup",
      "tags": ["a2a", "agent-to-agent", "vertex-ai", "multi-agent", "google-cloud"],
      "requiredAgents": ["google-a2a-expert"],
      "context7": [
        "/googleapis/google-cloud-python/vertexai",
        "/google-cloud/agent-builder",
        "/google-cloud/a2a-protocol"
      ]
    },
    {
      "name": "huggingface-deploy",
      "file": "commands/huggingface-deploy.md",
      "description": "Deploy HuggingFace models to production with quantization, vLLM high-throughput serving, and Inference Endpoints for 75% memory savings and 6x speedup",
      "category": "deployment",
      "tags": ["huggingface", "deployment", "quantization", "vllm", "inference-endpoints"],
      "requiredAgents": ["huggingface-expert"],
      "context7": [
        "/huggingface/transformers",
        "/huggingface/inference-endpoints",
        "/huggingface/quantization"
      ]
    }
  ],
  "rules": [
    {
      "name": "ai-model-standards",
      "file": "rules/ai-model-standards.md",
      "priority": "high",
      "description": "Comprehensive AI model development standards with Context7-verified best practices: model selection, async operations, error handling, caching, monitoring across OpenAI, HuggingFace, MLflow",
      "tags": [
        "ai",
        "standards",
        "best-practices",
        "async",
        "monitoring"
      ],
      "appliesTo": [
        "commands",
        "agents"
      ],
      "enforcesOn": [
        "openai-python-expert",
        "gemini-api-expert",
        "langgraph-workflow-expert"
      ]
    },
    {
      "name": "prompt-engineering-standards",
      "file": "rules/prompt-engineering-standards.md",
      "priority": "high",
      "description": "Comprehensive prompt engineering standards with Context7-verified best practices: system prompts, few-shot learning, chain-of-thought, structured outputs, RAG prompts, injection prevention",
      "tags": [
        "prompts",
        "engineering",
        "standards",
        "cot",
        "few-shot"
      ],
      "appliesTo": [
        "commands",
        "agents"
      ],
      "enforcesOn": [
        "openai-python-expert",
        "gemini-api-expert",
        "langgraph-workflow-expert"
      ]
    }
  ],
  "scripts": [
    {
      "name": "openai-chat-example",
      "file": "scripts/examples/openai-chat-example.py",
      "description": "OpenAI chat example demonstrating Context7 patterns: AsyncOpenAI, streaming, function calling, error handling with retry, response caching",
      "type": "example",
      "executable": true,
      "category": "ai",
      "tags": [
        "openai",
        "chat",
        "example",
        "async",
        "context7"
      ]
    },
    {
      "name": "langchain-rag-example",
      "file": "scripts/examples/langchain-rag-example.py",
      "description": "LangChain RAG example demonstrating Context7 patterns: RunnablePassthrough.assign(), MMR retrieval, document chunking, vector stores, RAG chains",
      "type": "example",
      "executable": true,
      "category": "ai",
      "tags": [
        "langchain",
        "rag",
        "example",
        "retrieval",
        "context7"
      ]
    },
    {
      "name": "huggingface-inference-example",
      "file": "scripts/examples/huggingface-inference-example.py",
      "description": "HuggingFace inference example demonstrating Context7 patterns: pipeline API, AutoTokenizer, AutoModel, device management, efficient batch processing",
      "type": "example",
      "executable": true,
      "category": "ai",
      "tags": [
        "huggingface",
        "inference",
        "example",
        "transformers",
        "context7"
      ]
    },
    {
      "name": "mlflow-tracking-example",
      "file": "scripts/examples/mlflow-tracking-example.py",
      "description": "MLflow tracking example demonstrating Context7 patterns: experiment tracking, parameter/metric logging, model registry, run comparison, artifact storage",
      "type": "example",
      "executable": true,
      "category": "ai",
      "tags": [
        "mlflow",
        "tracking",
        "example",
        "mlops",
        "context7"
      ]
    }
  ],
  "features": {
    "anthropic_integration": {
      "enabled": true,
      "description": "Anthropic Claude API with prompt caching, tool use, and vision"
    },
    "openai_integration": {
      "enabled": true,
      "description": "OpenAI GPT models, embeddings, and function calling"
    },
    "azure_openai_integration": {
      "enabled": true,
      "description": "Azure OpenAI Service with enterprise features and compliance"
    },
    "gemini_integration": {
      "enabled": true,
      "description": "Google Gemini multimodal AI with safety controls"
    },
    "langchain_framework": {
      "enabled": true,
      "description": "LangChain LCEL, chains, agents, memory, and RAG patterns"
    },
    "langgraph_workflows": {
      "enabled": true,
      "description": "LangGraph stateful AI workflows and multi-agent systems"
    },
    "a2a_protocol": {
      "enabled": true,
      "description": "Google Agent-to-Agent protocol with Vertex AI Agent Builder"
    },
    "huggingface_ecosystem": {
      "enabled": true,
      "description": "HuggingFace Transformers, Datasets, quantization, and deployment"
    },
    "rag_systems": {
      "enabled": true,
      "description": "Retrieval-Augmented Generation with LangChain and optimization"
    },
    "model_deployment": {
      "enabled": true,
      "description": "Production AI model deployment with vLLM, TGI, and FastAPI"
    },
    "prompt_engineering": {
      "enabled": true,
      "description": "Comprehensive prompt engineering standards and patterns"
    },
    "mlops_tracking": {
      "enabled": true,
      "description": "ML experiment tracking and model registry with MLflow"
    }
  },
  "dependencies": [],
  "peerPlugins": [
    "@claudeautopm/plugin-core"
  ],
  "mcpServers": {
    "recommended": [
      "context7"
    ],
    "optional": []
  },
  "keywords": [
    "claudeautopm",
    "plugin",
    "ai",
    "ml",
    "machine-learning",
    "anthropic",
    "claude",
    "openai",
    "azure-openai",
    "gemini",
    "langchain",
    "langgraph",
    "huggingface",
    "transformers",
    "a2a",
    "agent-to-agent",
    "vertex-ai",
    "mlflow",
    "rag",
    "llm",
    "gpt",
    "embeddings",
    "prompt-engineering",
    "mlops",
    "quantization",
    "vllm",
    "tool-use",
    "function-calling",
    "prompt-caching",
    "context7"
  ],
  "compatibleWith": ">=3.0.0"
}
